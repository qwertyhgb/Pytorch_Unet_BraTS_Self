"""
BraTS2021 è„‘è‚¿ç˜¤åˆ†å‰²è®­ç»ƒè„šæœ¬

åŠŸèƒ½ï¼š
    1. åŠ è½½ BraTS2021 æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†
    2. ä½¿ç”¨ U-Net 2D æ¨¡å‹è¿›è¡Œè®­ç»ƒ
    3. è®¡ç®—æŸå¤±å‡½æ•°ï¼ˆCrossEntropyLossï¼‰å’Œè¯„ä¼°æŒ‡æ ‡ï¼ˆDice Scoreï¼‰
    4. æ”¯æŒæ¨¡å‹ä¿å­˜ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœç­‰åŠŸèƒ½
    5. è®°å½•è®­ç»ƒæ—¥å¿—å’Œå¯è§†åŒ–

è®­ç»ƒé…ç½®ï¼š
    - æ¨¡å‹ï¼šU-Net 2D (4 è¾“å…¥é€šé“ â†’ 4 è¾“å‡ºç±»åˆ«)
    - æŸå¤±å‡½æ•°ï¼šCrossEntropyLossï¼ˆå¯é€‰ç±»åˆ«æƒé‡ï¼‰
    - ä¼˜åŒ–å™¨ï¼šAdam (lr=1e-4)
    - è¯„ä¼°æŒ‡æ ‡ï¼šDice Scoreï¼ˆé’ˆå¯¹ 3 ç±»è‚¿ç˜¤åŒºåŸŸï¼‰
    - è®­ç»ƒè½®æ•°ï¼š30 epochs
    - æ‰¹æ¬¡å¤§å°ï¼š8
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from dataset import BraTSDataset
from models.unet2d import UNet2D  # ä¿®æ­£å¯¼å…¥è·¯å¾„
from tqdm import tqdm
import os
import argparse
from datetime import datetime
import json


# ==================== é…ç½®å‚æ•° ====================

def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        argparse.Namespace: åŒ…å«æ‰€æœ‰è®­ç»ƒé…ç½®çš„å‚æ•°å¯¹è±¡
    """
    parser = argparse.ArgumentParser(description="BraTS2021 U-Net 2D è®­ç»ƒè„šæœ¬")
    
    # æ•°æ®ç›¸å…³
    parser.add_argument("--data_dir", type=str, default="./data_png",
                        help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--train_ratio", type=float, default=0.8,
                        help="è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.8ï¼‰")
    
    # æ¨¡å‹ç›¸å…³
    parser.add_argument("--in_channels", type=int, default=4,
                        help="è¾“å…¥é€šé“æ•°ï¼ˆ4 ç§ MRI æ¨¡æ€ï¼‰")
    parser.add_argument("--num_classes", type=int, default=4,
                        help="è¾“å‡ºç±»åˆ«æ•°ï¼ˆèƒŒæ™¯ + 3 ç±»è‚¿ç˜¤ï¼‰")
    parser.add_argument("--base_channels", type=int, default=64,
                        help="U-Net åŸºç¡€é€šé“æ•°")
    parser.add_argument("--bilinear", action="store_true",
                        help="ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·ï¼ˆé»˜è®¤ä½¿ç”¨è½¬ç½®å·ç§¯ï¼‰")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=30,
                        help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=8,
                        help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-4,
                        help="åˆå§‹å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-5,
                        help="æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰")
    parser.add_argument("--num_workers", type=int, default=2,
                        help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    parser.add_argument("--mixed_precision", action="store_true",
                        help="ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰åŠ é€Ÿ")
    parser.add_argument("--compile_model", action="store_true",
                        help="ä½¿ç”¨ torch.compile ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰")
    parser.add_argument("--gradient_accumulation", type=int, default=1,
                        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿæ›´å¤§çš„ batch sizeï¼‰")
    
    # æŸå¤±å‡½æ•°ç›¸å…³
    parser.add_argument("--use_class_weights", action="store_true",
                        help="ä½¿ç”¨ç±»åˆ«æƒé‡å¤„ç†ç±»åˆ«ä¸å¹³è¡¡")
    parser.add_argument("--dice_weight", type=float, default=0.0,
                        help="Dice Loss æƒé‡ï¼ˆ0 è¡¨ç¤ºåªä½¿ç”¨ CE Lossï¼‰")
    
    # å­¦ä¹ ç‡è°ƒåº¦
    parser.add_argument("--use_scheduler", action="store_true",
                        help="ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
    parser.add_argument("--scheduler_patience", type=int, default=5,
                        help="å­¦ä¹ ç‡è°ƒåº¦å™¨çš„è€å¿ƒå€¼")
    parser.add_argument("--scheduler_factor", type=float, default=0.5,
                        help="å­¦ä¹ ç‡è¡°å‡å› å­")
    
    # æ—©åœ
    parser.add_argument("--early_stopping", action="store_true",
                        help="å¯ç”¨æ—©åœæœºåˆ¶")
    parser.add_argument("--patience", type=int, default=10,
                        help="æ—©åœè€å¿ƒå€¼ï¼ˆéªŒè¯é›†æ— æ”¹å–„çš„è½®æ•°ï¼‰")
    
    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--save_dir", type=str, default="./checkpoints",
                        help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--log_interval", type=int, default=10,
                        help="æ—¥å¿—æ‰“å°é—´éš”ï¼ˆæ‰¹æ¬¡æ•°ï¼‰")
    parser.add_argument("--save_best_only", action="store_true",
                        help="åªä¿å­˜æœ€ä½³æ¨¡å‹")
    parser.add_argument("--resume", type=str, default=None,
                        help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­")
    parser.add_argument("--validate_every", type=int, default=1,
                        help="æ¯éš”å¤šå°‘è½®è¿›è¡Œä¸€æ¬¡éªŒè¯")
    
    return parser.parse_args()


# ==================== å·¥å…·å‡½æ•° ====================

def set_seed(seed):
    """
    è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡å¤
    
    Args:
        seed (int): éšæœºç§å­å€¼
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    import numpy as np
    import random
    np.random.seed(seed)
    random.seed(seed)


def dice_score(pred, target, num_classes=4, eps=1e-5):
    """
    è®¡ç®— Dice Scoreï¼ˆé’ˆå¯¹å¤šç±»åˆ«åˆ†å‰²ï¼‰- ä¼˜åŒ–ç‰ˆæœ¬
    
    Dice Score æ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè¡¡é‡é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾çš„é‡å ç¨‹åº¦
    å…¬å¼ï¼šDice = 2 * |X âˆ© Y| / (|X| + |Y|)
    
    Args:
        pred (Tensor): æ¨¡å‹é¢„æµ‹ logits [B, C, H, W]
        target (Tensor): çœŸå®æ ‡ç­¾ [B, H, W]ï¼Œå€¼ä¸º {0, 1, 2, 3}
        num_classes (int): ç±»åˆ«æ•°ï¼ˆé»˜è®¤ 4ï¼‰
        eps (float): å¹³æ»‘é¡¹ï¼Œé¿å…é™¤é›¶
    
    Returns:
        float: å¹³å‡ Dice Scoreï¼ˆåªè®¡ç®—è‚¿ç˜¤ç±»åˆ« 1, 2, 3ï¼Œå¿½ç•¥èƒŒæ™¯ï¼‰
    """
    # å°† logits è½¬æ¢ä¸ºç±»åˆ«é¢„æµ‹
    pred = torch.argmax(pred, dim=1)  # [B, H, W]
    
    # å‘é‡åŒ–è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„ Dice Score
    dice_scores = []
    for c in range(1, num_classes):
        pred_c = (pred == c)
        target_c = (target == c)
        intersection = (pred_c & target_c).sum().float()
        union = pred_c.sum().float() + target_c.sum().float()
        dice = (2.0 * intersection + eps) / (union + eps)
        dice_scores.append(dice.item())
    
    return sum(dice_scores) / len(dice_scores)


def dice_loss(pred, target, num_classes=4, eps=1e-5):
    """
    è®¡ç®— Dice Lossï¼ˆå¯ä¸ CrossEntropyLoss ç»“åˆä½¿ç”¨ï¼‰
    
    Dice Loss = 1 - Dice Score
    
    Args:
        pred (Tensor): æ¨¡å‹é¢„æµ‹ logits [B, C, H, W]
        target (Tensor): çœŸå®æ ‡ç­¾ [B, H, W]
        num_classes (int): ç±»åˆ«æ•°
        eps (float): å¹³æ»‘é¡¹
    
    Returns:
        Tensor: Dice Loss æ ‡é‡
    """
    # å°† logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    pred_probs = torch.softmax(pred, dim=1)  # [B, C, H, W]
    
    # å°† target è½¬æ¢ä¸º one-hot ç¼–ç 
    target_one_hot = torch.nn.functional.one_hot(target, num_classes)  # [B, H, W, C]
    target_one_hot = target_one_hot.permute(0, 3, 1, 2).float()  # [B, C, H, W]
    
    dice_loss_value = 0.0
    
    # éå†æ¯ä¸ªç±»åˆ«ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
    for c in range(num_classes):
        pred_c = pred_probs[:, c, :, :]  # [B, H, W]
        target_c = target_one_hot[:, c, :, :]  # [B, H, W]
        
        intersection = (pred_c * target_c).sum()
        union = pred_c.sum() + target_c.sum()
        
        dice = (2.0 * intersection + eps) / (union + eps)
        dice_loss_value += (1.0 - dice)
    
    return dice_loss_value / num_classes


def save_checkpoint(model, optimizer, epoch, best_dice, save_path, scheduler=None):
    """
    ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
    
    Args:
        model (nn.Module): æ¨¡å‹
        optimizer (Optimizer): ä¼˜åŒ–å™¨
        epoch (int): å½“å‰è½®æ•°
        best_dice (float): æœ€ä½³ Dice Score
        save_path (str): ä¿å­˜è·¯å¾„
        scheduler (optional): å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_dice': best_dice,
    }
    
    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()
    
    torch.save(checkpoint, save_path)
    print(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")


def load_checkpoint(model, optimizer, checkpoint_path, scheduler=None):
    """
    åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹
    
    Args:
        model (nn.Module): æ¨¡å‹
        optimizer (Optimizer): ä¼˜åŒ–å™¨
        checkpoint_path (str): æ£€æŸ¥ç‚¹è·¯å¾„
        scheduler (optional): å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Returns:
        tuple: (start_epoch, best_dice)
    """
    checkpoint = torch.load(checkpoint_path)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    start_epoch = checkpoint['epoch'] + 1
    best_dice = checkpoint.get('best_dice', 0.0)
    
    print(f"âœ“ æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")
    print(f"  æ¢å¤è®­ç»ƒä» Epoch {start_epoch}ï¼Œæœ€ä½³ Dice: {best_dice:.4f}")
    
    return start_epoch, best_dice


# ==================== è®­ç»ƒå’ŒéªŒè¯å‡½æ•° ====================

def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, args, scaler=None):
    """
    è®­ç»ƒä¸€ä¸ª epochï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        model (nn.Module): æ¨¡å‹
        train_loader (DataLoader): è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion (nn.Module): æŸå¤±å‡½æ•°
        optimizer (Optimizer): ä¼˜åŒ–å™¨
        device (torch.device): è®¾å¤‡
        epoch (int): å½“å‰è½®æ•°
        args (Namespace): è®­ç»ƒå‚æ•°
        scaler (GradScaler, optional): æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾å™¨
    
    Returns:
        tuple: (å¹³å‡æŸå¤±, å¹³å‡ Dice Score)
    """
    model.train()
    
    total_loss = 0.0
    total_dice = 0.0
    num_batches = len(train_loader)
    
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{args.epochs} [è®­ç»ƒ]")
    
    optimizer.zero_grad()  # åœ¨å¾ªç¯å¤–åˆå§‹åŒ–
    
    for batch_idx, (images, masks) in enumerate(pbar):
        # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ï¼ˆnon_blocking åŠ é€Ÿï¼‰
        images = images.to(device, non_blocking=True)
        masks = masks.to(device, non_blocking=True)
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if scaler is not None:
            with torch.cuda.amp.autocast():
                outputs = model(images)
                ce_loss = criterion(outputs, masks)
                
                if args.dice_weight > 0:
                    d_loss = dice_loss(outputs, masks, num_classes=args.num_classes)
                    loss = ce_loss + args.dice_weight * d_loss
                else:
                    loss = ce_loss
                
                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / args.gradient_accumulation
            
            scaler.scale(loss).backward()
            
            # æ¯ N æ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
            if (batch_idx + 1) % args.gradient_accumulation == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
        else:
            # æ ‡å‡†è®­ç»ƒ
            outputs = model(images)
            ce_loss = criterion(outputs, masks)
            
            if args.dice_weight > 0:
                d_loss = dice_loss(outputs, masks, num_classes=args.num_classes)
                loss = ce_loss + args.dice_weight * d_loss
            else:
                loss = ce_loss
            
            loss = loss / args.gradient_accumulation
            loss.backward()
            
            if (batch_idx + 1) % args.gradient_accumulation == 0:
                optimizer.step()
                optimizer.zero_grad()
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        with torch.no_grad():
            batch_dice = dice_score(outputs, masks, num_classes=args.num_classes)
        
        # ç´¯è®¡ç»Ÿè®¡
        total_loss += loss.item() * args.gradient_accumulation
        total_dice += batch_dice
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f'{loss.item() * args.gradient_accumulation:.4f}',
            'dice': f'{batch_dice:.4f}',
            'lr': f'{optimizer.param_groups[0]["lr"]:.6f}'
        })
    
    # è®¡ç®—å¹³å‡å€¼
    avg_loss = total_loss / num_batches
    avg_dice = total_dice / num_batches
    
    return avg_loss, avg_dice


@torch.no_grad()
def validate(model, val_loader, criterion, device, epoch, args):
    """
    éªŒè¯æ¨¡å‹æ€§èƒ½
    
    Args:
        model (nn.Module): æ¨¡å‹
        val_loader (DataLoader): éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion (nn.Module): æŸå¤±å‡½æ•°
        device (torch.device): è®¾å¤‡
        epoch (int): å½“å‰è½®æ•°
        args (Namespace): è®­ç»ƒå‚æ•°
    
    Returns:
        tuple: (å¹³å‡æŸå¤±, å¹³å‡ Dice Score)
    """
    model.eval()
    
    total_loss = 0.0
    total_dice = 0.0
    num_batches = len(val_loader)
    
    pbar = tqdm(val_loader, desc=f"Epoch {epoch}/{args.epochs} [éªŒè¯]")
    
    for images, masks in pbar:
        # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
        images = images.to(device)
        masks = masks.to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(images)
        
        # è®¡ç®—æŸå¤±
        ce_loss = criterion(outputs, masks)
        
        if args.dice_weight > 0:
            d_loss = dice_loss(outputs, masks, num_classes=args.num_classes)
            loss = ce_loss + args.dice_weight * d_loss
        else:
            loss = ce_loss
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        batch_dice = dice_score(outputs, masks, num_classes=args.num_classes)
        
        # ç´¯è®¡ç»Ÿè®¡
        total_loss += loss.item()
        total_dice += batch_dice
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'dice': f'{batch_dice:.4f}'
        })
    
    # è®¡ç®—å¹³å‡å€¼
    avg_loss = total_loss / num_batches
    avg_dice = total_dice / num_batches
    
    return avg_loss, avg_dice


# ==================== ä¸»è®­ç»ƒæµç¨‹ ====================

def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    # è§£æå‚æ•°
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"GPU å‹å·: {torch.cuda.get_device_name(0)}")
        print(f"GPU æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_path = os.path.join(args.save_dir, "train_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(vars(args), f, indent=4, ensure_ascii=False)
    print(f"âœ“ è®­ç»ƒé…ç½®å·²ä¿å­˜: {config_path}")
    
    # ==================== åŠ è½½æ•°æ®é›† ====================
    print("\n" + "=" * 60)
    print("åŠ è½½æ•°æ®é›†")
    print("=" * 60)
    
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†
    full_dataset = BraTSDataset(args.data_dir, validate_data=True)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(args.train_ratio * len(full_dataset))
    val_size = len(full_dataset) - train_size
    
    train_dataset, val_dataset = random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(args.seed)  # ç¡®ä¿å¯é‡å¤
    )
    
    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬ ({args.train_ratio * 100:.0f}%)")
    print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬ ({(1 - args.train_ratio) * 100:.0f}%)")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True if torch.cuda.is_available() else False,
        persistent_workers=True if args.num_workers > 0 else False,  # ä¿æŒ worker è¿›ç¨‹å­˜æ´»
        prefetch_factor=2 if args.num_workers > 0 else None  # é¢„å–æ•°æ®
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True if torch.cuda.is_available() else False,
        persistent_workers=True if args.num_workers > 0 else False,
        prefetch_factor=2 if args.num_workers > 0 else None
    )
    
    print(f"\næ•°æ®åŠ è½½å™¨:")
    print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    
    # ==================== åˆ›å»ºæ¨¡å‹ ====================
    print("\n" + "=" * 60)
    print("åˆ›å»ºæ¨¡å‹")
    print("=" * 60)
    
    model = UNet2D(
        in_ch=args.in_channels,
        out_ch=args.num_classes,
        bilinear=args.bilinear,
        base_ch=args.base_channels
    ).to(device)
    
    # ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
    if args.compile_model:
        try:
            print("æ­£åœ¨ç¼–è¯‘æ¨¡å‹...")
            model = torch.compile(model)
            print("âœ“ æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
        except Exception as e:
            print(f"âš  æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
            print("  ç»§ç»­ä½¿ç”¨æœªç¼–è¯‘çš„æ¨¡å‹")
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ¨¡å‹: U-Net 2D")
    print(f"  è¾“å…¥é€šé“: {args.in_channels}")
    print(f"  è¾“å‡ºç±»åˆ«: {args.num_classes}")
    print(f"  åŸºç¡€é€šé“: {args.base_channels}")
    print(f"  ä¸Šé‡‡æ ·æ–¹å¼: {'åŒçº¿æ€§æ’å€¼' if args.bilinear else 'è½¬ç½®å·ç§¯'}")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    if args.mixed_precision:
        print(f"  æ··åˆç²¾åº¦: å¯ç”¨ (FP16)")
    if args.gradient_accumulation > 1:
        print(f"  æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation} æ­¥ (ç­‰æ•ˆ batch size: {args.batch_size * args.gradient_accumulation})")
    
    # ==================== åˆ›å»ºæŸå¤±å‡½æ•° ====================
    if args.use_class_weights:
        # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
        print("\næ­£åœ¨è®¡ç®—ç±»åˆ«æƒé‡...")
        stats = full_dataset.get_statistics()
        class_weights = torch.tensor(stats['class_weights'], dtype=torch.float32).to(device)
        print(f"ç±»åˆ«æƒé‡: {class_weights.tolist()}")
        criterion = nn.CrossEntropyLoss(weight=class_weights)
    else:
        criterion = nn.CrossEntropyLoss()
    
    # ==================== åˆ›å»ºä¼˜åŒ–å™¨ ====================
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    # ==================== åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ ====================
    scheduler = None
    if args.use_scheduler:
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',  # ç›‘æ§ Dice Scoreï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
            factor=args.scheduler_factor,
            patience=args.scheduler_patience,
            verbose=True
        )
        print(f"\nå­¦ä¹ ç‡è°ƒåº¦å™¨: ReduceLROnPlateau")
        print(f"  è€å¿ƒå€¼: {args.scheduler_patience}")
        print(f"  è¡°å‡å› å­: {args.scheduler_factor}")
    
    # ==================== æ··åˆç²¾åº¦è®­ç»ƒ ====================
    scaler = None
    if args.mixed_precision and torch.cuda.is_available():
        scaler = torch.cuda.amp.GradScaler()
        print("\nâœ“ æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
    
    # ==================== æ¢å¤è®­ç»ƒï¼ˆå¯é€‰ï¼‰====================
    start_epoch = 1
    best_dice = 0.0
    patience_counter = 0
    
    if args.resume:
        start_epoch, best_dice = load_checkpoint(model, optimizer, args.resume, scheduler)
    
    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("\n" + "=" * 60)
    print("å¼€å§‹è®­ç»ƒ")
    print("=" * 60)
    
    train_history = {
        'train_loss': [],
        'train_dice': [],
        'val_loss': [],
        'val_dice': [],
        'lr': []
    }
    
    for epoch in range(start_epoch, args.epochs + 1):
        print(f"\n{'=' * 60}")
        print(f"Epoch {epoch}/{args.epochs}")
        print(f"{'=' * 60}")
        
        # è®­ç»ƒé˜¶æ®µ
        train_loss, train_dice = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch, args, scaler
        )
        
        # éªŒè¯é˜¶æ®µï¼ˆæ¯éš” validate_every è½®è¿›è¡Œä¸€æ¬¡ï¼‰
        if epoch % args.validate_every == 0:
            val_loss, val_dice = validate(
                model, val_loader, criterion, device, epoch, args
            )
        else:
            val_loss, val_dice = 0.0, 0.0
        
        # è®°å½•å†å²
        train_history['train_loss'].append(train_loss)
        train_history['train_dice'].append(train_dice)
        train_history['val_loss'].append(val_loss)
        train_history['val_dice'].append(val_dice)
        train_history['lr'].append(optimizer.param_groups[0]['lr'])
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'=' * 60}")
        print(f"Epoch {epoch} æ€»ç»“:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒ Dice: {train_dice:.4f}")
        if epoch % args.validate_every == 0:
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯ Dice: {val_dice:.4f}")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        print(f"{'=' * 60}")
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None and epoch % args.validate_every == 0:
            scheduler.step(val_dice)
        
        # ä¿å­˜æ¨¡å‹
        if epoch % args.validate_every == 0:
            is_best = val_dice > best_dice
            
            if is_best:
                best_dice = val_dice
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_model_path = os.path.join(args.save_dir, "best_model.pth")
                save_checkpoint(model, optimizer, epoch, best_dice, best_model_path, scheduler)
                print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ï¼Dice Score: {best_dice:.4f}")
            else:
                patience_counter += 1
            
            # ä¿å­˜æœ€æ–°æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            if not args.save_best_only:
                latest_model_path = os.path.join(args.save_dir, "latest_model.pth")
                save_checkpoint(model, optimizer, epoch, best_dice, latest_model_path, scheduler)
        
        # æ—©åœæ£€æŸ¥
        if args.early_stopping and patience_counter >= args.patience:
            print(f"\nâš  æ—©åœè§¦å‘ï¼éªŒè¯é›† Dice Score å·² {args.patience} è½®æœªæ”¹å–„")
            print(f"æœ€ä½³ Dice Score: {best_dice:.4f}")
            break
    
    # ==================== è®­ç»ƒå®Œæˆ ====================
    print("\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"æœ€ä½³éªŒè¯ Dice Score: {best_dice:.4f}")
    print(f"æ¨¡å‹ä¿å­˜ç›®å½•: {args.save_dir}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_path = os.path.join(args.save_dir, "train_history.json")
    with open(history_path, 'w') as f:
        json.dump(train_history, f, indent=4)
    print(f"âœ“ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


if __name__ == "__main__":
    main()
